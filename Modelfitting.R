library(gplots)
library(ROCR)
library(lattice)
library(ggplot2)
library(caret)
library(dplyr)

final<- read.csv("C:/Users/mrame/OneDrive/Desktop/Labelling files/finalexamDemoDiet.csv", stringsAsFactors = F)
str(final)
final2 <- final %>% mutate_if(is.character, as.factor)
final2<-final2[,-c(1,2)]
str(final2)
ncol(final)
library(FactoMineR)
sample_size <- floor(0.80*nrow(final))
set.seed(123)
train_ind <- sample(seq_len(nrow(final2)), size = sample_size)
train <- final2[train_ind,]
test <- final2[-train_ind,]
write.csv(train,"train.csv")
write.csv(test,"test.csv")
#Logistic
train <- read.csv('C:/Users/mrame/OneDrive/Desktop/Labelling files/train_PCA.csv')
train <- train[-1]
test <- read.csv('C:/Users/mrame/OneDrive/Desktop/Labelling files/test_PCA.csv')
test <- test[-1]
model1 <- glm(is_diabetic~.,family = binomial(link = 'logit'), data = train)
x_predict <- predict(model1,test,type = "response")
library(ROCR)
pr <- ROCR::prediction(x_predict,test$is_diabetic)
prf <- performance(pr,measure = "tpr",final.measure = "fpr")
plot(prf, main = " lr_ROC Curve", ylab = " Sensitivity" , xlab = "1-Specificity")
auc <- performance(pr, "auc")
auc<- unlist(slot(auc, "y.values"))
auc <- round(auc,4)
legend(.2,.2,auc, title = "AUC")
defaulted.pred <- ifelse(x_predict > 0.5,1,0)
conf_matrix <- table(defaulted.pred,test$is_diabetic)
library('caret')
confusionMatrix(conf_matrix)
# naive bayes model
library('e1071')
naive_bayes_model <- naiveBayes(as.factor(is_diabetic)~., data = train)
nb_predict <- predict(naive_bayes_model, newdata = test,type = "class")
conf_matrix_nb <- table(nb_predict,test$is_diabetic)
confusionMatrix(conf_matrix_nb)
plot(naive_bayes_model)
# SVM model
svm_model <- svm(is_diabetic~.,data = train)
svm_predict <- predict(svm_model,newdata = test)
library(ROCR)
pr_svm <- ROCR::prediction(svm_predict,test$is_diabetic)
prf_svm <- performance(pr_svm,measure = "tpr",x.measure = "fpr")
plot(prf_svm,main = " SVM_ROC Curve", ylab = " Sensitivity" , xlab = "1-Specificity")
abline(a=0,b=1)
auc <- performance(pr_svm, "auc")
auc<- unlist(slot(auc, "y.values"))
auc <- round(auc,4)
legend(.8,.2,auc, title = "AUC")
default.pred_svm3 <- ifelse(svm_predict > 0.4, 1, 0)
conf_matrix_svm <- table(default.pred_svm3,test$is_diabetic)
confusionMatrix(conf_matrix_svm)
# KNN model

normalize <- function(x){
  return ((x-min(x))/(max(x)-min(x)))
}
str(train)
norm_train <- as.data.frame(lapply(train[-c(1,3,4,5)], normalize))
norm_test <- as.data.frame(lapply(test[-c(1,3,4,5)], normalize))
dim(norm_train)
dim(norm_test)
model_knn <- train(norm_train[,-3],as.factor(norm_train[,3]),method = 'knn')
predict_knn <- predict(object = model_knn, norm_test[,-3])
conf_matrix_knn <- table(predict_knn,norm_test[,3])
confusionMatrix(conf_matrix_knn)

# random forest model
install.packages('randomForest')
library('randomForest')
is.na(final)
str(final)
summary(final2)
str(final2)
library(dplyr)
rf <- randomForest(as.factor(is_diabetic)~., ntree=100, data = train, importance=TRUE)
rf
varImpPlot(rf)
predict_rf <- predict(rf,test)
View(predict_rf)
conf_martix_rf <- table(predict_rf, test$is_diabetic)
confusionMatrix(conf_martix_rf)
# Neural Networks
install.packages('nnet')
library('nnet')
library(ROCR)
library(gplots)
lm.fit <- glm(is_diabetic~., data = train)
pr.lm <- predict(lm.fit, test[-9])
MSE.lm <- sum((pr.lm - test$is_diabetic)^2)/nrow(test)
apply(train, 2, function(x) sum(is.na(x)))
final3 <- final2[, sapply(final2, is.numeric)]
maxs <- apply(final3, 2, max)
mins <- apply(final3, 2, min)
scaled <- as.data.frame(scale(final3, center = mins, scale = maxs - mins))
index <- sample(1:nrow(final3),round(0.80*nrow(final3)))
train_new <- scaled[index,]
test_new <- scaled[-index,]
install.packages('neuralnet')
library(neuralnet)
n = names(train_new)
f = as.formula( paste( "is_diabetic ~", paste( n[!n %in% c("is_diabetic","SEQN")], collapse = "+" ) ) )
nn = neuralnet( f, train_new, hidden = 3, linear.output = FALSE, threshold = 0.5 ) 
plot( nn, rep = "best" )
colnames(test_new[,-3])
predictNN <- neuralnet::compute(nn, test_new[,-3])
tableNN<- table(round( predictNN$net.result ),test_new$is_diabetic )
confusionMatrix(tableNN)
#gradient boost
install.packages('gbm')
library('gbm')
library(caret)
gbm_model = gbm(formula = is_diabetic~.,distribution = "bernoulli",data = train,n.trees = 2500,shrinkage = .01,n.minobsinnode = 20, cv.folds = 5)
best.iter <- gbm.perf(gbm_model, method = "cv")
best.iter
summary(gbm_model)
predict_gbm <- predict(object = gbm_model,newdata = test,n.trees = 1500,type = "response")
conf_matrix_gbm <- data.frame("Actual"= test$is_diabetic, "predicted" = predict_gbm)
library(ROCR)
h <- getElement(test,"is_diabetic")
g<- as.data.frame(t(test$is_diabetic))
pr_gbm <- prediction(predict_gbm,test[,7])
prf_gbm <- performance(pr_gbm,measure = "tpr",x.measure = "fpr")
plot(prf_gbm,main = " gbm_ROC Curve", ylab = " Sensitivity" , xlab = "1-Specificity")
auc <- performance(pr_gbm, "auc")
auc<- unlist(slot(auc, "y.values"))
auc <- round(auc,4)
legend(.8,.2,auc, title = "AUC")
default.pred_gbm <- ifelse(predict_gbm > 0.4,1,0)
conf_matrix_gbm1 <- table(default.pred_gbm,test[,7])
confusionMatrix(conf_matrix_gbm1)
# Decision Tree
install.packages('rattle')
install.packages('RColorBrewer')
library('rattle')
library('RColorBrewer')
dectree <- rpart(is_diabetic~., data = train, method = "class", parms = list(split = "information"))
summary(dectree)
fancyRpartPlot(dectree)
preddecTree <- predict(dectree, test, type = "class")
conf_matrix_dectree <- table(preddecTree,test$is_diabetic)
confusionMatrix(conf_matrix_dectree)